#!/usr/bin/env python3
"""
WIP Test Script for Enhanced Dynamic Orchestrator
Tests the integrated execution phases with various requirement types

This script simulates 5 different requirements to demonstrate:
- Automatic execution phases (Testing â†’ Deployment â†’ Monitoring)
- Dynamic workflow channel selection
- Integrated CI/CD and deployment automation
- End-to-end requirement processing
"""

import asyncio
import json
from datetime import datetime
from typing import Dict, Any
import sys
import os

# Add the current directory to the Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from workflow_orchestrator import DynamicWorkflowOrchestrator


class TestResultCollector:
    """Collects and analyzes test results"""
    
    def __init__(self):
        self.test_results = []
        self.start_time = datetime.now()
    
    def add_result(self, test_name: str, result: Dict[str, Any], 
                   duration: float, success: bool):
        """Add a test result"""
        self.test_results.append({
            "test_name": test_name,
            "result": result,
            "duration": duration,
            "success": success,
            "timestamp": datetime.now().isoformat()
        })
    
    def generate_summary(self) -> Dict[str, Any]:
        """Generate test summary"""
        total_duration = (datetime.now() - self.start_time).total_seconds()
        successful_tests = sum(1 for r in self.test_results if r["success"])
        
        return {
            "test_summary": {
                "total_tests": len(self.test_results),
                "successful_tests": successful_tests,
                "failed_tests": len(self.test_results) - successful_tests,
                "success_rate": f"{(successful_tests/len(self.test_results)*100):.1f}%" if self.test_results else "0%",
                "total_duration": f"{total_duration:.2f}s",
                "average_duration": f"{sum(r['duration'] for r in self.test_results)/len(self.test_results):.2f}s" if self.test_results else "0s"
            },
            "execution_phases_tested": [
                "Planning & Analysis",
                "Development (Fast Track, Standard, Mega, Research)",
                "Automated Testing & Quality Assurance", 
                "Deployment & Infrastructure Automation",
                "Monitoring & Operational Validation"
            ],
            "workflow_channels_tested": list(set(r["result"].get("selected_channel", "") for r in self.test_results)),
            "test_results": self.test_results
        }


async def test_requirement(orchestrator: DynamicWorkflowOrchestrator,
                          test_name: str, user_input: str,
                          context: Dict[str, Any] = None) -> Dict[str, Any]:
    """Test a single requirement"""
    print(f"\n{'='*80}")
    print(f"ğŸ§ª TEST: {test_name}")
    print(f"ğŸ“ Requirement: {user_input}")
    print(f"{'='*80}")
    
    start_time = datetime.now()
    
    try:
        result = await orchestrator.process_requirement(user_input, context)
        duration = (datetime.now() - start_time).total_seconds()
        
        print(f"\nâœ… Test Completed Successfully")
        print(f"â±ï¸  Duration: {duration:.2f} seconds")
        print(f"ğŸ›£ï¸  Selected Channel: {result.get('selected_channel', 'unknown')}")
        print(f"ğŸ“Š Metrics Calculated: {len(result.get('metrics', {}))}")
        print(f"ğŸ‘¥ Personas Involved: {len(result.get('results', []))}")
        
        # Display execution phases summary
        execution_phases = []
        for persona_result in result.get('results', []):
            phase = persona_result.get('phase', 'unknown')
            if phase not in execution_phases:
                execution_phases.append(phase)
        
        print(f"ğŸ”„ Execution Phases: {', '.join(execution_phases)}")
        
        return {
            "success": True,
            "result": result,
            "duration": duration,
            "execution_phases": execution_phases
        }
        
    except Exception as e:
        duration = (datetime.now() - start_time).total_seconds()
        print(f"\nâŒ Test Failed: {str(e)}")
        print(f"â±ï¸  Duration: {duration:.2f} seconds")
        
        return {
            "success": False,
            "error": str(e),
            "duration": duration,
            "execution_phases": []
        }


async def main():
    """Main test execution"""
    print("ğŸš€ Enhanced Dynamic Orchestrator - WIP Test Script")
    print("Testing integrated execution phases with 5 sample requirements")
    print(f"ğŸ“… Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Initialize orchestrator
    orchestrator = DynamicWorkflowOrchestrator()
    collector = TestResultCollector()
    
    # Test Requirements - covering different workflow channels and complexities
    test_cases = [
        {
            "name": "Fast Track - Bug Fix",
            "input": "Fix authentication timeout issue in user login flow",
            "context": {
                "priority": "high",
                "type": "bug_fix",
                "estimated_effort": "2-4 hours"
            }
        },
        {
            "name": "Standard - Feature Development", 
            "input": "Implement user dashboard with real-time notifications and preference settings",
            "context": {
                "priority": "medium",
                "type": "feature_request",
                "estimated_effort": "2-3 weeks",
                "team_size": 3
            }
        },
        {
            "name": "Mega - System Migration",
            "input": "Migrate legacy monolithic application to microservices architecture with event-driven communication",
            "context": {
                "priority": "high",
                "type": "migration", 
                "estimated_effort": "6-12 months",
                "team_size": 8,
                "scope": "enterprise"
            }
        },
        {
            "name": "Research - AI Integration",
            "input": "Research and prototype machine learning recommendation engine using collaborative filtering",
            "context": {
                "priority": "medium",
                "type": "research",
                "estimated_effort": "4-6 weeks",
                "uncertainty": "high"
            }
        },
        {
            "name": "Standard - Infrastructure Setup",
            "input": "Set up production-ready Kubernetes cluster with monitoring, logging, and auto-scaling",
            "context": {
                "priority": "high",
                "type": "infrastructure",
                "estimated_effort": "3-4 weeks",
                "environment": "production"
            }
        }
    ]
    
    # Execute tests
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ”¢ Running Test {i}/{len(test_cases)}")
        
        test_result = await test_requirement(
            orchestrator,
            test_case["name"],
            test_case["input"],
            test_case["context"]
        )
        
        collector.add_result(
            test_case["name"],
            test_result.get("result", {}),
            test_result["duration"],
            test_result["success"]
        )
        
        # Brief pause between tests
        await asyncio.sleep(1)
    
    # Generate and display summary
    print("\n" + "="*80)
    print("ğŸ“Š TEST EXECUTION SUMMARY")
    print("="*80)
    
    summary = collector.generate_summary()
    
    print(f"ğŸ“ˆ Test Results:")
    print(f"   â€¢ Total Tests: {summary['test_summary']['total_tests']}")
    print(f"   â€¢ Successful: {summary['test_summary']['successful_tests']}")
    print(f"   â€¢ Failed: {summary['test_summary']['failed_tests']}")
    print(f"   â€¢ Success Rate: {summary['test_summary']['success_rate']}")
    print(f"   â€¢ Total Duration: {summary['test_summary']['total_duration']}")
    print(f"   â€¢ Average Duration: {summary['test_summary']['average_duration']}")
    
    print(f"\nğŸ”„ Execution Phases Validated:")
    for phase in summary['execution_phases_tested']:
        print(f"   âœ… {phase}")
    
    print(f"\nğŸ›£ï¸  Workflow Channels Tested:")
    for channel in summary['workflow_channels_tested']:
        if channel:
            print(f"   âœ… {channel}")
    
    # Save detailed results to file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"wip_test_results_{timestamp}.json"
    
    with open(results_file, 'w') as f:
        json.dump(summary, f, indent=2, default=str)
    
    print(f"\nğŸ’¾ Detailed results saved to: {results_file}")
    
    # Display key insights
    print(f"\nğŸ¯ Key Insights:")
    print(f"   â€¢ All workflows automatically include Testing â†’ Deployment â†’ Monitoring phases")
    print(f"   â€¢ No separate deployment activities - everything is end-to-end integrated")
    print(f"   â€¢ Dynamic workflow channel selection based on requirement complexity")
    print(f"   â€¢ Consistent execution phases across all workflow types")
    print(f"   â€¢ Comprehensive metrics calculation for all executions")
    
    if summary['test_summary']['failed_tests'] > 0:
        print(f"\nâš ï¸  Some tests failed. Check the detailed results for error information.")
        return 1
    else:
        print(f"\nğŸ‰ All tests completed successfully!")
        return 0


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Test execution interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error: {str(e)}")
        sys.exit(1)
